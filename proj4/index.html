<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CS 180 Portfolio: Project 4</title>
    <link rel="stylesheet" href="../style.css" />
  </head>
  <body>
    <header>
      <h1>Alex Luu's CS 180 Project Portfolio</h1>
      <h2>Project 4: Neural Radiance Field</h2>
    </header>
    <main class="project-content">
      <p class="back-home"><a href="../index.html">&larr; Back to Home</a></p>

      <div class="section">
        <h2>Part 0.1: Calibrating Your Camera</h2>
        <p>
          I captured 43 images of ArUco calibration tags from my phone camera at
          various angles and distances, keeping the zoom level consistent. For
          each image, I detected the ArUco tags using OpenCV's ArUco detector,
          extracted the corner coordinates, and collected all detected corners
          with their corresponding 3D world coordinates. Using
          <code>cv2.calibrateCamera()</code>, I computed the camera intrinsics
          and distortion coefficients. If no tags were detected in an image, I
          excluded that image from the calibration dataset.
        </p>
        <p>
          For efficiency, I downsampled the images so it would train faster.
          This involves downsampling the calibration images in this section
          before doing any calculations. To be consistent, I also downsampled
          the images I took of my custom object.
        </p>

        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part0/example_calibration.jpeg"
                alt="Example Calibration Image"
                style="max-height: 400px"
              />
              <p>Example calibration image showing ArUco tags</p>
            </div>
          </div>
        </div>
      </div>

      <div class="section">
        <h2>Part 0.2: Capturing a 3D Object Scan</h2>
        <p>
          I captured 47 images of a camping lamp with a single ArUco tag placed
          next to it on a tabletop, using the same camera and zoom level as the
          calibration. The images were taken from different angles around the
          object, ensuring the tag was visible in each shot.
        </p>

        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part0/example_object.jpeg"
                alt="Example Object Image"
                style="max-height: 400px"
              />
              <p>Example camping lamp image with ArUco tag</p>
            </div>
          </div>
        </div>
      </div>

      <div class="section">
        <h2>Part 0.3: Estimating Camera Pose</h2>
        <p>
          Using the camera intrinsics and distortion coefficients from Part 0.1,
          I estimated the camera pose (position and orientation) for each image
          of the object scan. This is the classic Perspective-n-Point (PnP)
          problem: given a set of 3D points in world coordinates and their
          corresponding 2D projections in an image, find the camera's extrinsic
          parameters (rotation and translation).
        </p>
        <p>
          For each image, I detected the single ArUco tag and used
          <code>cv2.solvePnP()</code> to estimate the camera pose. The function
          requires the 3D coordinates of the ArUco tag corners in world space,
          the detected 2D pixel coordinates of the tag corners in the image, and
          the camera intrinsic matrix and distortion coefficients.
        </p>
        <p>
          OpenCV's <code>solvePnP()</code> returns the world-to-camera
          transformation. I inverted this to get the camera-to-world (c2w)
          matrix for visualization and later use in NeRF training.
        </p>

        <h3>Camera Frustum Visualization</h3>
        <p>
          I visualized the camera poses using Viser, which displays camera
          frustums in 3D space showing both their positions and the captured
          images. Below are two screenshots of the visualization.
        </p>

        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part0/visualization1.jpeg"
                alt="Camera Frustum Visualization 1"
                style="max-height: 500px"
              />
              <p>Viser visualization (view 1)</p>
            </div>
            <div>
              <img
                src="images/part0/visualization2.jpeg"
                alt="Camera Frustum Visualization 2"
                style="max-height: 500px"
              />
              <p>Viser visualization (view 2)</p>
            </div>
          </div>
        </div>
      </div>

      <div class="section">
        <h2>Part 0.4: Undistorting Images and Creating a Dataset</h2>
        <p>
          With the camera intrinsics, distortion coefficients, and pose
          estimates, the final step is to undistort the images and package
          everything into a dataset for NeRF training.
        </p>
        <p>
          I used <code>cv2.undistort()</code> to remove lens distortion from the
          images. After undistorting all images, I split them into training,
          validation, and test sets (90% train, 10% validation).
        </p>
      </div>

      <div style="margin: 80px"></div>
      <hr />
      <div style="margin: 30px"></div>

      <div class="section">
        <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
        <p>
          In this section, I implemented a Neural Field that represents a 2D
          image. The network takes in 2D pixel coordinates (x, y) and outputs
          the 3D RGB color at that position.
        </p>

        <h3>Network Architecture</h3>
        <p>
          I implemented a Multilayer Perceptron (MLP) network with Sinusoidal
          Positional Encoding (PE). The MLP consists of a stack of fully
          connected layers with ReLU activations, followed by a sigmoid
          activation at the output to constrain RGB values to [0, 1]. The
          network structure is:
        </p>
        <ul>
          <li>
            <strong>Input:</strong> 2D pixel coordinates (x, y) normalized to
            [0, 1]
          </li>
          <li>
            <strong>Positional Encoding:</strong> Expands the 2-dim coordinates
            to a higher dimensional space using sinusoidal functions
          </li>
          <li>
            <strong>MLP Layers:</strong> Linear(PE_dim, width) → ReLU →
            Linear(width, width) → ReLU → Linear(width, width) → ReLU →
            Linear(width, 3) → Sigmoid
          </li>
          <li>
            <strong>Output:</strong> 3-dim RGB pixel colors in range [0, 1]
          </li>
        </ul>
        <p>
          The Positional Encoding applies sinusoidal functions to expand the
          dimensionality: PE(x) = [x, sin(2^0πx), cos(2^0πx), sin(2^1x),
          cos(2^1πx), ..., sin(2^(L-1)πx), cos(2^(L-1)πx)], where L is the
          maximum frequency level. This maps a 2D coordinate to a (4L +
          2)-dimensional vector.
        </p>

        <h3>Training Setup</h3>
        <p>
          I implemented a dataloader that randomly samples N pixels per
          iteration for training. The model is trained using Mean Squared Error
          (MSE) loss with the Adam optimizer (learning rate 1e-2) for 2000
          iterations with a batch size of 10k pixels. I used PSNR (Peak
          Signal-to-Noise Ratio) as the evaluation metric: PSNR = 10 × log₁₀(1 /
          MSE).
        </p>

        <h3>Hyperparameter Tuning</h3>
        <p>
          I experimented with different values of L (positional encoding
          frequency) and width (number of channels in hidden layers) to see
          their effect on reconstruction quality. Below is a 2×2 grid comparing
          results with L ∈ {2, 10} and width ∈ {32, 256} on the staff image:
        </p>

        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part1/staff_image_L2_w32.jpg"
                alt="Staff Image L=2, w=32"
              />
              <p>L=2, width=32</p>
            </div>
            <div>
              <img
                src="images/part1/staff_image_L2_w256.jpg"
                alt="Staff Image L=2, w=256"
              />
              <p>L=2, width=256</p>
            </div>
          </div>
          <div class="flex-row">
            <div>
              <img
                src="images/part1/staff_image_L10_w32.jpg"
                alt="Staff Image L=10, w=32"
              />
              <p>L=10, width=32</p>
            </div>
            <div>
              <img
                src="images/part1/staff_image_L10_w256.jpg"
                alt="Staff Image L=10, w=256"
              />
              <p>L=10, width=256</p>
            </div>
          </div>
        </div>

        <p>
          From these results, higher L values capture more high-frequency
          details, while wider networks have more capacity. The best
          configuration I found was L=12 and width=384, which I used for the
          final results below.
        </p>

        <h3>Training Progression</h3>
        <p>
          Below are snapshots of the network's reconstruction at different
          training iterations, showing how the model progressively learns to
          represent the image details. The images show results at iterations
          100, 250, 500, and 1500 for both the staff image and the Palace of
          Fine Arts image.
        </p>

        <h4>Staff Image Training Progression (L=12, width=384)</h4>
        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part1/staff_image_L12_w384_iter100.jpg"
                alt="Staff Image iter 100"
              />
              <p>Iteration 100</p>
            </div>
            <div>
              <img
                src="images/part1/staff_image_L12_w384_iter250.jpg"
                alt="Staff Image iter 250"
              />
              <p>Iteration 250</p>
            </div>
          </div>
          <div class="flex-row">
            <div>
              <img
                src="images/part1/staff_image_L12_w384_iter500.jpg"
                alt="Staff Image iter 500"
              />
              <p>Iteration 500</p>
            </div>
            <div>
              <img
                src="images/part1/staff_image_L12_w384_iter1500.jpg"
                alt="Staff Image iter 1500"
              />
              <p>Iteration 1500</p>
            </div>
          </div>
        </div>

        <h4>Palace of Fine Arts Training Progression (L=12, width=384)</h4>
        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part1/pofa_image_L12_w384_iter100.jpg"
                alt="Palace of Fine Arts iter 100"
              />
              <p>Iteration 100</p>
            </div>
            <div>
              <img
                src="images/part1/pofa_image_L12_w384_iter250.jpg"
                alt="Palace of Fine Arts iter 250"
              />
              <p>Iteration 250</p>
            </div>
          </div>
          <div class="flex-row">
            <div>
              <img
                src="images/part1/pofa_image_L12_w384_iter500.jpg"
                alt="Palace of Fine Arts iter 500"
              />
              <p>Iteration 500</p>
            </div>
            <div>
              <img
                src="images/part1/pofa_image_L12_w384_iter1500.jpg"
                alt="Palace of Fine Arts iter 1500"
              />
              <p>Iteration 1500</p>
            </div>
          </div>
        </div>

        <h3>PSNR Curves</h3>
        <p>
          The graphs below show the PSNR (Peak Signal-to-Noise Ratio) during
          training. Higher PSNR indicates better reconstruction quality.
        </p>

        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part1/staff_image_L12_w384_psnr.jpg"
                alt="Staff Image PSNR"
              />
              <p>Staff Image PSNR Curve</p>
            </div>
            <div>
              <img
                src="images/part1/pofa_image_L12_w384_psnr.jpg"
                alt="Palace of Fine Arts PSNR"
              />
              <p>Palace of Fine Arts PSNR Curve</p>
            </div>
          </div>
        </div>

        <h3>Final Results</h3>
        <p>
          Below are the original images and final reconstructed images after
          2000 iterations with L=12, width=384, and same architecture as the
          provided network:
        </p>

        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part1/staff_image.jpg"
                alt="Staff Image Original"
              />
              <p>Staff Image (Original)</p>
            </div>
            <div>
              <img
                src="images/part1/palace_of_fine_arts.jpeg"
                alt="Palace of Fine Arts Original"
              />
              <p>Palace of Fine Arts (Original)</p>
            </div>
          </div>
          <div class="flex-row">
            <div>
              <img
                src="images/part1/staff_image_L12_w384.jpg"
                alt="Staff Image Final"
              />
              <p>Staff Image (Final Reconstruction)</p>
            </div>
            <div>
              <img
                src="images/part1/pofa_image_L12_w384.jpg"
                alt="Palace of Fine Arts Final"
              />
              <p>Palace of Fine Arts (Final Reconstruction)</p>
            </div>
          </div>
        </div>
      </div>

      <div style="margin: 80px"></div>
      <hr />
      <div style="margin: 30px"></div>

      <div class="section">
        <h2>Part 2.1: Create Rays from Cameras</h2>
        <p>
          To render a 3D scene from multiple camera viewpoints, I need to
          convert pixel coordinates into 3D rays in world space. This involves
          three coordinate transformations. I choose to implement this using
          batched coordinates and c2w matrices for efficiency.
        </p>

        <h3>Camera to World Coordinate Conversion</h3>
        <p>
          The transformation between world space and camera space is defined by
          the camera-to-world (c2w) transformation matrix, which consists of a
          rotation matrix R and translation vector t. I implemented the function
          <code>x_w = transform(c2w, x_c)</code> to transform points from camera
          coordinates to world coordinates. Because of the batched c2w and
          points, I used np.einsum for this to correcly handle the dimensions
        </p>

        <h3>Pixel to Camera Coordinate Conversion</h3>
        <p>
          I implemented <code>x_c = pixel_to_camera(K, uv, s)</code> to convert
          the pixel coordinates to camera coordinates using the inverse of the
          intrinsic matrix K at a given depth s. I used simple numpy operations
          for this part to inverse the matrix and perform the matrix
          multiplication.
        </p>

        <h3>Pixel to Ray Conversion</h3>
        <p>
          A ray is defined by an origin r_o and direction r_d. For a pinhole
          camera, the ray origin is simply the camera position in world space
          (the translation component of the c2w matrix). To find the ray
          direction, I choose a point along the ray at depth s = 1 in camera
          space, convert it to world coordinates, and compute the normalized
          direction vector: r_d = (Xw - r_o) / ||Xw - r_0||_2.
        </p>
      </div>

      <div class="section">
        <h2>Part 2.2: Sampling</h2>
        <p>
          With the ability to convert pixels to rays, I now need to sample rays
          from the training images and sample 3D points along those rays.
        </p>

        <h3>Sampling Rays from Images</h3>
        <p>
          For training, I need to sample N rays at every iteration. I
          implemented the global sampling where I flattened all the pixels from
          all training images and sampled N rays globally. To flatten the
          pixels, I use np.meshgrid and duplicated the c2w matrices to ensure
          the same dimensions as the pixels. I then used pixel_to_ray to convert
          the sampled pixels to rays.
        </p>

        <h3>Sampling Points Along Rays</h3>
        <p>
          After obtaining rays, I need to discretize each ray into sample points
          in 3D space. For a ray defined by r(t) = r_o + t·r_d, I sample points
          at regular intervals between near and far bounds. I added a parameter
          to optionally perturb the sample locations with random noise to
          improve training stability. I choose to use 64 samples per ray.
        </p>
      </div>

      <div class="section">
        <h2>Part 2.3: Putting the Dataloading All Together</h2>
        <p>
          I combined all the components from Parts 2.1 and 2.2 into a complete
          dataloader for NeRF training. The dataloader randomly samples rays
          from the training set of images and c2w matrices. I also implemented a
          function that takes in a batch from the dataloader and returns the
          sampled 3D points along each ray, along with their corresponding RGB
          colors from the images using the sampling function in part 2.2. This
          would be the data fed into the NeRF network during training.
        </p>

        <h3>Ray and Sample Visualization</h3>
        <p>
          Below are visualizations showing the rays and sample points in 3D
          space at a single training step. The visualization plots 100 rays with
          their sample points.
        </p>

        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lego_rays_visual1.jpeg"
                alt="Ray Visualization 1"
                style="max-height: 500px"
              />
              <p>Ray and sample visualization (view 1)</p>
            </div>
            <div>
              <img
                src="images/part2/lego_rays_visual2.jpeg"
                alt="Ray Visualization 2"
                style="max-height: 500px"
              />
              <p>Ray and sample visualization (view 2)</p>
            </div>
          </div>
        </div>
      </div>

      <div class="section">
        <h2>Part 2.4: Neural Radiance Field</h2>
        <p>
          The Neural Radiance Field (NeRF) network extends the 2D neural field
          from Part 1 to represent a 3D scene. The key difference is that the
          network now takes 3D coordinates and view directions (r_d) as input
          and outputs both color and density.
        </p>

        <h3>Network Architecture</h3>
        <p>The NeRF network has three main changes from Part 1:</p>
        <ul>
          <li>
            <strong>Input:</strong> 3D world coordinates (x, y, z) along with
            the 3D ray direction (d_x, d_y, d_z). Both are encoded using
            positional encoding with different frequency levels.
          </li>
          <li>
            <strong>Output:</strong> RGB color (3 values) and density σ (1
            value). The density must be positive, so it is constrained using
            ReLU to ensure non-negative values. The RGB is constrined using
            softmax.
          </li>
          <li>
            <strong>Modified Structure:</strong> The network is deeper to handle
            the complexity. The 3D coordinates go through many fully connected
            layers and has a concatenation with the original input (after PE).
            It then splits into two branches: one for predicting density σ and
            another for predicting color c. The color branch takes the ray
            direction encoding through concatenation.
          </li>
        </ul>
        <p>
          For both the lego and custom renders below, I used the same exact
          architecture as the provided network except for the values of the
          positional encoding and width. I used PyTorch to implement this
          network.
        </p>
      </div>

      <div class="section">
        <h2>Part 2.5: Volume Rendering</h2>
        <p>
          In this section, I implemented the volume rendering function and
          trained the network on the Lego dataset.
        </p>

        <h3>Volume Rendering Function</h3>
        <p>
          The volume rendering equation combines the density and color
          predictions from the NeRF network along each ray to produce the final
          pixel color. I implemented the discrete volume rendering function:
          Ĉ(r) = Σᵢ Tᵢ(1 - exp(-σᵢδᵢ))cᵢ, where Tᵢ = exp(-Σⱼ₌₁ⁱ⁻¹ σⱼδⱼ). Since
          this function is part of the backpropagation, I used PyTorch tensor
          operations (e.g. torch.cumsum) and vectorization for speed.
        </p>

        <h3>Training Steps</h3>
        <p>
          For the training loop, I used the dataloader from Part 2.3 to sample N
          rays and their corresponding RGB colors. I then sampled 3D points
          along each ray and passed them through the NeRF network to get color
          and density predictions. From this, I reshaped the outputs back into
          groups of rays (shape [N, 64, 3 or 1]) and applied the volume
          rendering function to get the final pixel colors. I computed the MSE
          loss between the rendered colors and ground truth colors, and used the
          Adam optimizer to update the network weights.
        </p>
        <p>
          At regular intervals, I evaluated the model on the validation set by
          rendering full images using all rays from the validation cameras and
          computed the PSNR. I created validation and test helper functions to
          do this.
        </p>

        <h3>Training Progression</h3>
        <p>
          I trained the NeRF on the Lego dataset for 1200 iterations with a
          learning rate of 5e-4, Adam optimizer, and a batch size of 10k rays
          (so 64k inputs per iteration). I used a frequency of L=12 for the
          coordinates and L=6 for the direction vector. The width of the linear
          layers were 288. Below are validation images at different training
          stages.
        </p>

        <div class="flex-column">
          <h4 style="margin-bottom: 0">Iteration 50</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lego_val_iter50_1.jpg"
                alt="Lego Val Iter 50 Image 1"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter50_2.jpg"
                alt="Lego Val Iter 50 Image 2"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter50_3.jpg"
                alt="Lego Val Iter 50 Image 3"
                style="height: 300px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 100</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lego_val_iter100_1.jpg"
                alt="Lego Val Iter 100 Image 1"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter100_2.jpg"
                alt="Lego Val Iter 100 Image 2"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter100_3.jpg"
                alt="Lego Val Iter 100 Image 3"
                style="height: 300px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 250</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lego_val_iter250_1.jpg"
                alt="Lego Val Iter 250 Image 1"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter250_2.jpg"
                alt="Lego Val Iter 250 Image 2"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter250_3.jpg"
                alt="Lego Val Iter 250 Image 3"
                style="height: 300px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 500</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lego_val_iter500_1.jpg"
                alt="Lego Val Iter 500 Image 1"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter500_2.jpg"
                alt="Lego Val Iter 500 Image 2"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter500_3.jpg"
                alt="Lego Val Iter 500 Image 3"
                style="height: 300px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 800</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lego_val_iter800_1.jpg"
                alt="Lego Val Iter 800 Image 1"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter800_2.jpg"
                alt="Lego Val Iter 800 Image 2"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter800_3.jpg"
                alt="Lego Val Iter 800 Image 3"
                style="height: 300px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 1200</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lego_val_iter1200_1.jpg"
                alt="Lego Val Iter 1200 Image 1"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter1200_2.jpg"
                alt="Lego Val Iter 1200 Image 2"
                style="height: 300px"
              />
            </div>
            <div>
              <img
                src="images/part2/lego_val_iter1200_3.jpg"
                alt="Lego Val Iter 1200 Image 3"
                style="height: 300px"
              />
            </div>
          </div>
        </div>

        <h3>Metrics</h3>
        <p>
          Below are the PSNR for training and validation. The final training
          PSNR is 23.14. The final validation PSNR is 24.04.
        </p>
        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img src="images/part2/lego_psnr.jpg" alt="Lego PSNR Curve" />
            </div>
            <div>
              <img src="images/part2/lego_psnr_val.jpg" alt="Lego PSNR Val Curve" />
            </div>
          </div>
        </div>

        <h3>Final Novel View Rendering</h3>
        <p>
          After training, I rendered a spherical view around the Lego bulldozer
          using test camera poses.
        </p>

        <div class="flex-column">
          <div class="flex-row">
            <div style="text-align: center; width: 100%">
              <img
                src="images/part2/lego_render.gif"
                alt="Lego Novel View Rendering"
                style="height: 700px"
              />
            </div>
          </div>
        </div>
      </div>

      <div class="section">
        <h2>Part 2.6: Training with your own data</h2>
        <p>
          Using the camping lamp dataset I captured and processed in Part 0, I
          trained a NeRF model on my own data.
        </p>

        <h3>Model Architecture and Training Parameters</h3>
        <p>I used the following configuration for training the camping lamp:</p>
        <ul>
          <li><strong>Positional Encoding (coordinates):</strong> L_x = 12</li>
          <li>
            <strong>Positional Encoding (ray direction):</strong> L_rd = 5
          </li>
          <li><strong>Network Width:</strong> 300</li>
          A slightly larger width than the lego model to capture more details.
          <li><strong>Near-Far Bounds:</strong> 0.15 to 0.9 meters</li>
          <li><strong>Learning Rate:</strong> 2e-4</li>
          I found a smaller lr to be better since the model tended to overfit.
          <li><strong>Number of Iterations:</strong> 2800</li>
          <li><strong>Batch Size:</strong> 10k rays per iteration</li>
          <li><strong>Number of Samples Per Ray:</strong> 64</li>
        </ul>

        <h3>Training Progression</h3>
        <div class="flex-column">
          <h4 style="margin-bottom: 0">Iteration 600</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lamp_val_iter600_1.jpg"
                alt="Lamp Val Iter 600 Image 1"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter600_2.jpg"
                alt="Lamp Val Iter 600 Image 2"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter600_3.jpg"
                alt="Lamp Val Iter 600 Image 3"
                style="height: 350px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 1100</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lamp_val_iter1100_1.jpg"
                alt="Lamp Val Iter 1100 Image 1"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter1100_2.jpg"
                alt="Lamp Val Iter 1100 Image 2"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter1100_3.jpg"
                alt="Lamp Val Iter 1100 Image 3"
                style="height: 350px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 1600</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lamp_val_iter1600_1.jpg"
                alt="Lamp Val Iter 1600 Image 1"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter1600_2.jpg"
                alt="Lamp Val Iter 1600 Image 2"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter1600_3.jpg"
                alt="Lamp Val Iter 1600 Image 3"
                style="height: 350px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 2000</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lamp_val_iter2000_1.jpg"
                alt="Lamp Val Iter 2000 Image 1"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter2000_2.jpg"
                alt="Lamp Val Iter 2000 Image 2"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter2000_3.jpg"
                alt="Lamp Val Iter 2000 Image 3"
                style="height: 350px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 2400</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lamp_val_iter2400_1.jpg"
                alt="Lamp Val Iter 2400 Image 1"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter2400_2.jpg"
                alt="Lamp Val Iter 2400 Image 2"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter2400_3.jpg"
                alt="Lamp Val Iter 2400 Image 3"
                style="height: 350px"
              />
            </div>
          </div>

          <h4 style="margin-bottom: 0">Iteration 2800</h4>
          <div class="flex-row">
            <div>
              <img
                src="images/part2/lamp_val_iter2800_1.jpg"
                alt="Lamp Val Iter 2800 Image 1"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter2800_2.jpg"
                alt="Lamp Val Iter 2800 Image 2"
                style="height: 350px"
              />
            </div>
            <div>
              <img
                src="images/part2/lamp_val_iter2800_3.jpg"
                alt="Lamp Val Iter 2800 Image 3"
                style="height: 350px"
              />
            </div>
          </div>
        </div>

        <h3>Metrics</h3>
        <p>
          Below are the PSNR and MSE curves during training. The final training
          PSNR is 19.23 and the final validation PSNR is 14.02. While these
          result are lower than the lego dataset, the model does capture the
          overall shape and appearance of the lamp pretty well.
        </p>
        <div class="flex-column">
          <div class="flex-row">
            <div>
              <img src="images/part2/lamp_psnr.jpg" alt="Lamp PSNR Curve" />
            </div>
            <div>
              <img src="images/part2/lamp_mse.jpg" alt="Lamp MSE Curve" />
            </div>
          </div>
        </div>

        <h3>Final Novel View Rendering</h3>
        <div class="flex-column">
          <div class="flex-row">
            <div style="text-align: center; width: 100%">
              <img
                src="images/part2/lamp_render.gif"
                alt="Lamp Novel View Rendering"
                style="height: 900px"
              />
            </div>
          </div>
        </div>
        <p>
          The results of this rendering is clearly worse than the lego dataset.
          This is likely due to the training data. The camera images I captured
          had varying distances from the center which made it hard for the model
          to train. I tried many hyperparameters and near/far values and these
          were the best I could come up with. Additionally, there were some
          inconsistent lighting since I took these pictures in the morning. That
          probably explains the white artifacts in the rendering. I also found
          the model to overfit quickly, which made it harder to train for lots
          of iterations.
        </p>
        <p>
          Despite all of these challenges, the results are still impressive.
          There were no training images for the angle where the object blocks
          the tag, yet the rendering is able to infer a reasonable appearance
          for that area. It was especially impressive how the model even
          generated the ArUco tag through the clear part of the lamp in that
          angle.
        </p>
      </div>
    </main>
    <footer>
      <p>&copy; 2025 Alex Luu</p>
    </footer>
  </body>
</html>
